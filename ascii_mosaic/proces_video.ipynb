{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "from scipy.misc import imresize \n",
    "import moviepy.editor as moviepy\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "\n",
    "import AsciiMosaic #cvts all chars for pixel values \n",
    "import Text2Img #cvts specifics chars from text file to values \n",
    "\n",
    "# import importlib\n",
    "# importlib.reload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(value, leftMin, leftMax, rightMin, rightMax):\n",
    "    '''a function to map values from one domain to another\n",
    "    for converting the iteration index to a parameter for the crop selection'''\n",
    "    # Figure out how 'wide' each range is\n",
    "    leftSpan = leftMax - leftMin\n",
    "    rightSpan = rightMax - rightMin\n",
    "\n",
    "    # Convert the left range into a 0-1 range (float)\n",
    "    valueScaled = float(value - leftMin) / float(leftSpan)\n",
    "\n",
    "    # Convert the 0-1 range into a value in the right range.\n",
    "    return rightMin + (valueScaled * rightSpan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = [1080, 1920]\n",
    "\n",
    "video_in = 'obama.mp4'\n",
    "video_out = 'tester.avi'\n",
    "orig_movie = moviepy.VideoFileClip(video_in) \n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID') #codec \n",
    "writer = cv2.VideoWriter(video_out, fourcc, orig_movie.fps, tuple(size[::-1]), False)\n",
    "\n",
    "mosiac = Text2Img.Processor(10) #choose your processor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd69e45580874d89b3d61f905aacd127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scale = 100\n",
    "min_scale = 20\n",
    "min_dims = [int(x/2*min_scale/100) for x in size]\n",
    "shift = [0, 0]\n",
    "\n",
    "'''grab frame from video, process, resize, write to output'''\n",
    "for i in tnrange(int(orig_movie.fps * orig_movie.duration), desc='progress'): #\n",
    "    \n",
    "    #grab the next frame and process it \n",
    "    frame = orig_movie.get_frame(i/orig_movie.duration)\n",
    "    ascii_img = mosiac.process_image(imresize(frame, 200))\n",
    "    center = [int(x/2) for x in ascii_img.shape]\n",
    "    \n",
    "    #adjust frame parameters \n",
    "    if i <= 200: \n",
    "        '''scale down and push to left corner'''\n",
    "        scale = int(translate(i, 0, 200, 100, 20))\n",
    "        shift[0] = int(translate(i, 0, 200, 0, -(size[0]/2 - min_dims[0])))\n",
    "        shift[1] = int(translate(i, 0, 200, 0, -(size[1]/2 - min_dims[1])))\n",
    "        \n",
    "    elif i > 600 and i < 700: \n",
    "        '''keep scale and move to right corner'''\n",
    "        shift[1] = int(translate(i, 600, 700, -(size[1]/2 - min_dims[1]), size[1]/2 - min_dims[1]))\n",
    "        \n",
    "    if i > 700 and i < 900: \n",
    "        '''scale up and move to origin'''\n",
    "        scale = int(translate(i, 700, 900, 20, 100))\n",
    "        shift[0] = int(translate(i, 700, 900, -(size[0]/2 - min_dims[0]), 0))\n",
    "        shift[1] = int(translate(i, 700, 900, (size[1]/2 - min_dims[1]), 0))\n",
    "\n",
    "    #crop the selection determine by the above parameters \n",
    "    new_loc = [sum(x) for x in zip(center, shift)]\n",
    "    rect_size_x, rect_size_y = [int(x/2*scale/100) for x in size]\n",
    "\n",
    "    cropped = ascii_img[(new_loc[0] - rect_size_x):(new_loc[0] + rect_size_x),\n",
    "                   (new_loc[1] - rect_size_y):(new_loc[1] + rect_size_y)]\n",
    "              \n",
    "    writer.write(imresize(cropped, size).astype('u1'))\n",
    "    \n",
    "writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for stopping early \n",
    "writer.release()\n",
    "cv2.imwrite('test.png', ascii_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# '''grab frame from video, process, resize, write to output'''\n",
    "# for i in tnrange(int(orig_movie.fps * orig_movie.duration), desc='progress'): #\n",
    "#     frame = orig_movie.get_frame(i/orig_movie.duration)\n",
    "#     ascii_img = mosiac.process_image(imresize(frame, 1))\n",
    "#     writer.write(imresize(ascii_img, SIZE).astype('u1'))\n",
    "    \n",
    "# writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer.release()\n",
    "# cv2.imwrite('test.png', ascii_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pixels to Ascii Characters\n",
    "\n",
    "This code is inspired from the [BitsOfCode](https://bitesofcode.wordpress.com/2017/01/19/converting-images-to-ascii-art-part-1/) blog post on converting pixel values of an image to ascii characters. I created an image processor in [AsciiMosaic.py](AsciiMosaic.py) to do this efficiently, making use of numpy's `np.vectorize` function to vectorize iterating through the image and mapping the pixels to ascii characters. A rough workflow is explained in [test_ascii_mosiac_gif.ipynb](test_ascii_mosiac_gif.ipynb), where I convert an image to the ascii characters, and create a gif to zoom in on the detail. \n",
    "\n",
    "<img src=\"obama/obama_ascii.gif\" width=\"600\" alt=\"raw\" />\n",
    "\n",
    "I decided to go one step further and select specific text for the ascii character mapping. This was accomplished by getting the transcript of a speech and selecting the intensity of the background to match that of the pixel which is behind it. The process was actually simpler than the random character mapping, but maybe that is because I had solved random mapping already. The processor code is under [Text2Img.py](Text2Img.py). I also wrote a few scripts to zoom in on the detail for the output video. \n",
    "\n",
    "## VIDEO HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
